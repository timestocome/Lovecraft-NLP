{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# https://github.com/timestocome\n",
    "\n",
    "\n",
    "# Lovecraft Corpus\n",
    "# https://github.com/vilmibm/lovecraftcorpus\n",
    "\n",
    "\n",
    "# Conv network sorts stories with about 88% accuracy\n",
    "# Not enough data to pull out a hold out set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# silence is golden\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(action=\"ignore\",category=DeprecationWarning)\n",
    "warnings.filterwarnings(action=\"ignore\",category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# hack to make keras work with 2*** series gpus\n",
    "\n",
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all files under the input directory\n",
    "import os\n",
    "\n",
    "fNames = []\n",
    "for dirname, _, filenames in os.walk('lovecraftcorpus'):\n",
    "    for filename in filenames:\n",
    "        fNames.append(os.path.join(dirname, filename))\n",
    "\n",
    "#print(fNames)\n",
    "#print(len(fNames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all files, split into sentences, do a bit of cleanup to reduce vocabulary size\n",
    "\n",
    "# keep cleanup minimal \n",
    "#  convert to lower\n",
    "#  convert all numbers to 9\n",
    "#  remove \",'\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import functools\n",
    "import re\n",
    "\n",
    "story_sentences = []\n",
    "targets = []\n",
    "\n",
    "for i in range(len(fNames)):\n",
    "    f = fNames[i]\n",
    "    fp = open(f)\n",
    "    story = fp.read()\n",
    "    \n",
    "    # minor cleanup\n",
    "    story = story.lower()\n",
    "    story = re.sub('-', ' ', story)\n",
    "    story = re.sub(\" \\'\", ' ', story)\n",
    "    story = re.sub('\\\"', ' ', story)\n",
    "    story = re.sub('\\d', '9', story)\n",
    "    \n",
    "    # break into sentences and append to the story_sentences array\n",
    "    story_sentences.append(sent_tokenize(story))\n",
    "    \n",
    "    \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# split sentences into words and get story number for each sentence to use as the target\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "story_words = []\n",
    "targets = []\n",
    "\n",
    "for i in range(len(fNames)):\n",
    "    \n",
    "    sentences = story_sentences[i]\n",
    "    sentence_words = [word_tokenize(t) for t in sentences]\n",
    "    \n",
    "    targets.append( [i] * len(sentences))\n",
    "    story_words.append(sentence_words)\n",
    "\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxlen 344\n"
     ]
    }
   ],
   "source": [
    "     \n",
    "# break sentences into words\n",
    "combined_words = [item for sublist in story_words for item in sublist]  \n",
    "\n",
    "# flatten sentence words, reduce to unique and sort\n",
    "unique_words = sorted(set(x for s in combined_words for x in s))\n",
    "\n",
    "# flatten lists\n",
    "sentences = [item for sublist in story_sentences for item in sublist]  \n",
    "targets = [item for sublist in targets for item in sublist]  \n",
    "\n",
    "# max length of sentences\n",
    "maxlen = max([len(x) for x in combined_words])\n",
    "print('maxlen', maxlen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       target                                              words\n",
      "18673      66  [their, deeds, i, recall, not, ,, for, they, w...\n",
      "18674      66  [their, aspect, i, recall, dimly, ,, it, was, ...\n",
      "18675      66  [their, name, i, recall, clearly, ,, for, it, ...\n",
      "18676      66  [these, beings, of, yesterday, were, called, m...\n",
      "18677      66  [so, the, genie, flew, back, to, the, thin, ho...\n"
     ]
    }
   ],
   "source": [
    "# store sentences and targets in a df\n",
    "# df makes it easier to shuffle samples and pull out train/test data \n",
    "\n",
    "train = pd.DataFrame(targets)\n",
    "train.columns = ['target']\n",
    "train['words'] = combined_words\n",
    "\n",
    "print(train.tail())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    target                                              words  \\\n",
      "0        0  [beyond, the, wall, of, sleep, i, have, often,...   \n",
      "1        0  [whilst, the, greater, number, of, our, noctur...   \n",
      "2        0  [from, my, experience, i, can, not, doubt, but...   \n",
      "3        0  [from, those, blurred, and, fragmentary, memor...   \n",
      "4        0  [we, may, guess, that, in, dreams, life, ,, ma...   \n",
      "5        0  [sometimes, i, believe, that, this, less, mate...   \n",
      "6        0  [it, was, from, a, youthful, revery, filled, w...   \n",
      "7        0  [his, name, ,, as, given, on, the, records, ,,...   \n",
      "8        0  [among, these, odd, folk, ,, who, correspond, ...   \n",
      "9        0  [joe, slater, ,, who, came, to, the, instituti...   \n",
      "10       0  [though, well, above, the, middle, stature, ,,...   \n",
      "11       0  [his, age, was, unknown, ,, since, among, his,...   \n",
      "12       0  [from, the, medical, and, court, documents, we...   \n",
      "13       0  [he, had, habitually, slept, at, night, beyond...   \n",
      "14       0  [not, that, his, form, of, language, was, at, ...   \n",
      "15       0  [he, himself, was, generally, as, terrified, a...   \n",
      "16       0  [as, slater, grew, older, ,, it, appeared, ,, ...   \n",
      "17       0  [one, day, near, noon, ,, after, a, profound, ...   \n",
      "18       0  [rushing, out, into, the, snow, ,, he, had, fl...   \n",
      "19       0  [as, two, men, of, moderate, size, sought, to,...   \n",
      "\n",
      "                                              indexes  \n",
      "0   [2139, 20999, 23017, 14249, 19110, 10261, 9631...  \n",
      "1   [23292, 20999, 9206, 14079, 14249, 14503, 1393...  \n",
      "2   [8524, 13577, 7418, 10261, 2987, 14009, 6243, ...  \n",
      "3   [8524, 21115, 2348, 861, 8419, 12916, 23137, 1...  \n",
      "4   [23137, 12795, 9335, 20991, 10525, 6335, 12033...  \n",
      "5   [19369, 10261, 2044, 20991, 21102, 11963, 1274...  \n",
      "6   [11227, 23081, 8524, 105, 23803, 17500, 7925, ...  \n",
      "7   [9927, 13636, 59, 1296, 8919, 14323, 20999, 16...  \n",
      "8   [800, 21051, 14236, 8210, 59, 23353, 4580, 726...  \n",
      "9   [11337, 19085, 59, 23353, 2968, 21314, 20999, ...  \n",
      "10  [21118, 23207, 167, 20999, 13034, 19861, 59, 8...  \n",
      "11  [9927, 526, 23081, 22243, 59, 18961, 800, 9927...  \n",
      "12  [8524, 20999, 12856, 861, 4669, 6156, 23137, 1...  \n",
      "13  [9652, 9411, 9405, 19123, 1434, 13881, 2139, 2...  \n",
      "14  [14009, 20991, 9927, 8333, 14249, 11728, 23081...  \n",
      "15  [9652, 9903, 23081, 8796, 1296, 20942, 861, 17...  \n",
      "16  [1296, 19085, 9235, 14299, 59, 11227, 1052, 59...  \n",
      "17  [14328, 5186, 13716, 13972, 59, 516, 105, 1612...  \n",
      "18  [17875, 14507, 11067, 20999, 19269, 59, 9652, ...  \n",
      "19  [1296, 21869, 12924, 14249, 13235, 19015, 1942...  \n"
     ]
    }
   ],
   "source": [
    "# there are faster ways to do this but this is easily reversable, works for small dataset\n",
    "\n",
    "# convert words to int ids\n",
    "def convert_word(word_list):\n",
    "    \n",
    "    wl = [0] * maxlen\n",
    "    n = min(maxlen, len(word_list))\n",
    "    \n",
    "    for i in range(n):\n",
    "            wl[i] = unique_words.index(word_list[i])\n",
    "      \n",
    "    return wl\n",
    "\n",
    "\n",
    "# util to convert ints back into words  ** not used here, coded for use in future code using this as base\n",
    "def convert_index(word_list):\n",
    "    l = []\n",
    "    for w in word_list:\n",
    "        l.append(unique_words[w])\n",
    "        \n",
    "    return l\n",
    "\n",
    "\n",
    "train['indexes'] = train['words'].apply(convert_word)\n",
    "print(train.head(20))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a small dataset, wouldn't be practical to use all unique words in most problems\n",
    "# drop common words ( stop words ) and one-offs \n",
    "max_features = len(unique_words)\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "n_filters = 250\n",
    "kernel_size = 3\n",
    "n_hidden = 250\n",
    "n_epochs = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1867 16811\n",
      "(16811, 67) (1866, 67)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# convert words into oneHot vectors\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(train['target'].values.reshape(-1,1))\n",
    "\n",
    "\n",
    "# shuffle data\n",
    "train = train.sample(frac=1.)\n",
    "\n",
    "\n",
    "# pull out 10% as validation data\n",
    "n_test = len(train) // 10\n",
    "n_train = len(train) - n_test\n",
    "print(n_test, n_train)\n",
    "\n",
    "\n",
    "# split data into train/validate and\n",
    "# reshape pandas series into arrays for network\n",
    "# train\n",
    "x = train[0:n_train]['indexes']\n",
    "x = np.asarray([y for z in x for y in z])\n",
    "x = x.reshape(n_train, maxlen)\n",
    "\n",
    "y = train[0:n_train]['target'].values.reshape(-1,1)\n",
    "y = enc.transform(y)\n",
    "\n",
    "\n",
    "# test\n",
    "x_test = train[n_train:-1]['indexes']\n",
    "\n",
    "x_test = np.asarray([y for z in x_test for y in z])\n",
    "x_test = x_test.reshape(n_test-1, maxlen)\n",
    "\n",
    "\n",
    "y_test = train[n_train:-1]['target'].values.reshape(-1,1)\n",
    "y_test = enc.transform(y_test)\n",
    "\n",
    "print(y.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/herself/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/herself/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/herself/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/herself/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/herself/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/herself/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ML model word embedding layer, Conv 1D, Dense, Dense output \n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(Conv1D(n_filters, kernel_size, padding='valid', activation='relu', strides=1))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "\n",
    "model.add(Dense(n_hidden))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "model.add(Dense(len(fNames)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/herself/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 16811 samples, validate on 1866 samples\n",
      "Epoch 1/20\n",
      "16811/16811 [==============================] - 4s 223us/step - loss: 0.0139 - acc: 0.1330 - val_loss: 0.0127 - val_acc: 0.2412\n",
      "Epoch 2/20\n",
      "16811/16811 [==============================] - 2s 119us/step - loss: 0.0120 - acc: 0.2967 - val_loss: 0.0115 - val_acc: 0.3258\n",
      "Epoch 3/20\n",
      "16811/16811 [==============================] - 2s 127us/step - loss: 0.0103 - acc: 0.4044 - val_loss: 0.0109 - val_acc: 0.3730\n",
      "Epoch 4/20\n",
      "16811/16811 [==============================] - 2s 119us/step - loss: 0.0090 - acc: 0.4833 - val_loss: 0.0110 - val_acc: 0.3794\n",
      "Epoch 5/20\n",
      "16811/16811 [==============================] - 2s 128us/step - loss: 0.0080 - acc: 0.5396 - val_loss: 0.0109 - val_acc: 0.3901\n",
      "Epoch 6/20\n",
      "16811/16811 [==============================] - 2s 124us/step - loss: 0.0072 - acc: 0.5894 - val_loss: 0.0110 - val_acc: 0.3923\n",
      "Epoch 7/20\n",
      "16811/16811 [==============================] - 2s 127us/step - loss: 0.0064 - acc: 0.6390 - val_loss: 0.0109 - val_acc: 0.4126\n",
      "Epoch 8/20\n",
      "16811/16811 [==============================] - 2s 116us/step - loss: 0.0057 - acc: 0.6792 - val_loss: 0.0108 - val_acc: 0.4255\n",
      "Epoch 9/20\n",
      "16811/16811 [==============================] - 2s 131us/step - loss: 0.0052 - acc: 0.7142 - val_loss: 0.0108 - val_acc: 0.4405\n",
      "Epoch 10/20\n",
      "16811/16811 [==============================] - 2s 136us/step - loss: 0.0046 - acc: 0.7479 - val_loss: 0.0107 - val_acc: 0.4384\n",
      "Epoch 11/20\n",
      "16811/16811 [==============================] - 2s 121us/step - loss: 0.0042 - acc: 0.7794 - val_loss: 0.0108 - val_acc: 0.4432\n",
      "Epoch 12/20\n",
      "16811/16811 [==============================] - 2s 125us/step - loss: 0.0037 - acc: 0.8017 - val_loss: 0.0109 - val_acc: 0.4491\n",
      "Epoch 13/20\n",
      "16811/16811 [==============================] - 2s 124us/step - loss: 0.0034 - acc: 0.8233 - val_loss: 0.0111 - val_acc: 0.4582\n",
      "Epoch 14/20\n",
      "16811/16811 [==============================] - 2s 126us/step - loss: 0.0031 - acc: 0.8382 - val_loss: 0.0112 - val_acc: 0.4421\n",
      "Epoch 15/20\n",
      "16811/16811 [==============================] - 2s 131us/step - loss: 0.0029 - acc: 0.8493 - val_loss: 0.0113 - val_acc: 0.4427\n",
      "Epoch 16/20\n",
      "16811/16811 [==============================] - 2s 135us/step - loss: 0.0027 - acc: 0.8634 - val_loss: 0.0114 - val_acc: 0.4603\n",
      "Epoch 17/20\n",
      "16811/16811 [==============================] - 2s 121us/step - loss: 0.0025 - acc: 0.8710 - val_loss: 0.0114 - val_acc: 0.4539\n",
      "Epoch 18/20\n",
      "16811/16811 [==============================] - 2s 128us/step - loss: 0.0023 - acc: 0.8827 - val_loss: 0.0115 - val_acc: 0.4555\n",
      "Epoch 19/20\n",
      "16811/16811 [==============================] - 2s 125us/step - loss: 0.0022 - acc: 0.8866 - val_loss: 0.0116 - val_acc: 0.4523\n",
      "Epoch 20/20\n",
      "16811/16811 [==============================] - 2s 134us/step - loss: 0.0021 - acc: 0.8959 - val_loss: 0.0116 - val_acc: 0.4555\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbce054a780>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# train model \n",
    "\n",
    "model.fit(x, y,\n",
    "          batch_size = batch_size,\n",
    "          epochs = n_epochs,\n",
    "          validation_data = (x_test, y_test))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
